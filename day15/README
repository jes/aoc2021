Today's part 2 problem is too big to fit in RAM.

It consists of a 100x100 grid, which is tiled to 5x5 in total, creating a 500x500
grid, but each tiling changes the numbers slightly such that the shortest path
changes.

My plan was to use Floyd-Warshall (all-pairs shortest path) to solve
all 9 of the possible 100x100 grids that can be tiled,
store these solutions in some format that gives me the shortest path from any
point on the perimeter to any other point on the perimeter, and then stitch the
25 "perimeter-only" grids together into a graph of 10k (or 6k using Ben's scheme)
nodes that can then be solved in memory.

(Ben's idea is that since the join between 2 perimeters is just a single edge, you can
bake the cost of that edge into just 1 node. But maybe not worth it if it makes the
subsequent graph construction more complicated).

We have 25 grids arranged like this:

0 1 2 3 4
1 2 3 4 5
2 3 4 5 6
3 4 5 6 7
4 5 6 7 8

Grid number N always has grid number N-1 above and left, so Ben's scheme might work.

But a 100x100 grid has 400 points on its perimeter, so we would have 400x400 values
in each file = 160k, so this isn't going to work. If we stop double-counting the
corners, and self paths, and remove reverse paths, then we get 396*395/2 = 78k pairs,
which is still way too many.

----

Now I think the best idea is to use the disk to store the visited set and the queue,
and use most of the "real" memory just as a disk cache. The visited set can be a
bit-set of 15626 words, so perhaps it could go in memory. It's not obvious whether
we'd get more benefit from having the visited set in memory but a smaller cache of
the queue, or putting them both on disk and having more cache.

The kernel doesn't provide random access to files on disk, but we could manually
allocate however many blocks are needed, copy the cf_blkread/cf_blkwrite functions
into our program, and read/write the blocks on disk directly.

I think this is one to revisit after December.
